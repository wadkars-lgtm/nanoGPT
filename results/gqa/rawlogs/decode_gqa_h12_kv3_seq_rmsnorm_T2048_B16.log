Overriding: device = cuda:0
Overriding: dtype = bfloat16
Overriding: phase = decode
Overriding: kv_cache = True
Overriding: prompt_len = 2048
Overriding: batch_size = 16
Overriding: max_new_tokens = 1
Overriding: warmup_iters = 3
Overriding: bench_iters = 5
Overriding: n_head = 12
Overriding: n_kv_head = 3
Overriding: use_rope = False
Overriding: norm_type = rmsnorm
Overriding: norm_eps = 1e-05
Overriding: allow_unsafe_benchmark = True
Overriding: ignore_checkpoint = True
number of parameters: 113.06M
[INFO] Saved prefix cache: C:\Users\wadka\Documents\GitHub\nanoGPT\bench_prefix\prefix_16d7b202953e.pt
Wrote: bench_out\infer_h12_kv3_T2048_B16_KVtrue_Pdecode_rmsnorm_norope.json
Wrote: raw_logs\infer_h12_kv3_T2048_B16_KVtrue_Pdecode_rmsnorm_norope.log
params: {
  "batch_size": 16,
  "prompt_len": 2048,
  "max_new_tokens": 1,
  "kv_cache": true,
  "phase": "decode",
  "prefix_source": "random",
  "prefix_cache_dir": "bench_prefix",
  "cache_prefix": true,
  "prefix_seed_len": 1,
  "compile": false,
  "allow_unsafe_benchmark": true,
  "ignore_checkpoint": true,
  "block_size": 2049,
  "vocab_size": 50304,
  "n_layer": 12,
  "n_head": 12,
  "n_kv_head": 3,
  "n_embd": 768,
  "dropout": 0.0,
  "bias": true,
  "use_rope": false,
  "norm_type": "rmsnorm",
  "norm_eps": 1e-05,
  "ckpt_dir": "out-attn",
  "ckpt_name": ""
}
metrics: {
  "peak_alloc_bytes": 2397622272,
  "peak_reserved_bytes": 2982150144,
  "decode_total_ms": 473.76182250976564,
  "decode_ms_per_tok": 473.76182250976564,
  "decode_tok_s": 2.1108842361681517
}
